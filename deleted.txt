nid	mid	fields
1594231490281	1516129351266	Glasgow-Coma-Scale (GCS)	Dient der speziellen Einteilung des SHT (Schädelhirntrauma) und wird bei jeder Akutsituation zur Beurteilung des neurologischen Status verwendet.<div><br></div><div>Bewertet wird die Beeinträchtigung der Vigilanz durch Beobachten der Reaktionen des Patienten beim Augenöffnen, bei verbaler Kommunikation und motorischer Reaktion.</div><div><br></div><div><img src="paste-4040f361a5f3763a28444cd59610289d7abde1e9.jpg"><br></div>
1591013943737	1516129351266	Definition "Odds"	The odds are defined as the probability that the event will occur divided by the probability that the event will <i>not</i> occur.<div><br></div><div><b><u>Example</u></b></div><div>The odds of flipping a coin to heads is 50:50 = 1:1 = 1.0</div>
1591014164998	1516129351266	Methods for computing the CI of a proportion	&nbsp;- The <i>exact method</i> of Clopper and Pearson (<i>not</i> easily computed by hand)<div>&nbsp;- The <i>modified Wald method </i>(easily computed by hand)</div>
1591014333305	1516129351266	How to compute the <i>modified Wald method</i> by hand	<img src="paste-fa5a27dbebdf602226af7e35b0cc56b4b2937201.jpg">
1591014419798	1516129351266	The "rule of three"	<img src="paste-f84368ee695f345836bebe34d5e031f78c499c59.jpg">
1591014495770	1516129351266	Intention to treat	Each subject's survival is analyzed as if he or she received the assigned treatment, even if the treatment was not actually given.
1591014556554	1516129351266	Types of continuous variables	<b><u>Nominal</u></b><div>Discrete variables that can only have a set number of discrete possible values (e.g. color in psychology).</div><div><br></div><div>If there are only 2 possible values, the variable is called <i>binomial</i> (e.g. alive or dead).</div><div><br></div><div><b><u>Ordinal</u></b></div><div>Discrete variables that express rank. The order matters but not the exact value (e.g. pain on a VAS 1-10).</div><div><br></div><div>It doesn't make sense to compute differences or ratios. The median can be computed.</div><div><br></div><div><b><u>Interval</u></b></div><div>Continuous variables. The difference of 1 unit means the same thing all the way along the scale, no matter where you start (e.g. temperature in °C).</div><div><br></div><div>There is <i>no</i> non-arbitrary absolute zero. It doesn't make sense to compute ratios.</div><div><br></div><div><b><u>Ratio</u></b></div><div>Continuous variables. There <i>is</i> a non-arbitrary absolute zero (e.g. temperature in Kelvin). Ratios can be computed.</div><div><br></div><div><img src="paste-ccab30a2c69d98236eebfae0675df333878769fe.jpg"><br></div>
1591015230955	1516129351266	Rule of thumb for interpretation of a SD	About two-thirds of the observations in a population usually lie within the range defined by the mean minus 1 SD to the mean plus 1 SD.
1591015518175	1516129351266	Standard deviation (SD)	<img src="paste-ebcc93a5a429e3bd7c71f57999692216b91e254f.jpg">
1591015579261	1516129351266	Variance	The variance is equal to SD<sup>2</sup>
1591015656848	1516129351266	Why n - 1 instead of n when computing a SD?	<div>Except in the rare case in which the sample mean happens to equal the population mean, the data will be closer to the sample mean than they will be to the true population mean (because we are <i>squaring</i> the differences).<br></div><div>Therefore, the sum of squares will probably be a bit smaller (and can't be larger) than what it would be if you computed the difference between each value and the true population mean.</div><div>Because the numerator is a bit too small, the denominator must be made smaller too.</div><div><br></div><div>If you knew the sample mean and all but one of the values, you could calculate what that last value must be. Only n - 1 of the values are free to assume any value. Therefore, we calculate the average of the squared deviations by dividing by (n - 1) and say that there are n - 1 <b>degrees of freedom (df)</b>.</div><div><br></div><div><b><u>Exception</u></b></div>It sometimes makes sense to use n in the denominator instead of n - 1, when the goal is to quantify the variation in a particular set of data without any extrapolation to make wider conclusions.
1591016097893	1516129351266	Coefficient of variation (CV)	The CV equals the SD divided by the mean. It's a fraction with no units. It can only be computed for ratio variables.<div><div><br></div><div>The CV is useful for <i>comparing</i> scatter of variables measured in different units.</div></div>
1591016261276	1516129351266	The "five-number summary"	The distribution of a set of numbers can be summarized with five values, known as the <i>five-number summary</i>:<div><br></div><div>1) the minimum</div><div>2) the maximum</div><div>3) the 25th percentile (first quartile)</div><div>4) the 75th percentile (third quartile)</div><div>5) the median</div>
1591016369563	1516129351266	Median absolute deviation (MAD)	Compute the median of all values. Then calculate how far each value is from the median of all values and take the absolute of that difference. Now find the median of that set of differences. The result is the MAD.<br><div><br></div><div>The MAD is a simple way to quantify variation. It is the value such that half of the values are closer to the median than it and half are farther away.<br></div><div><br></div><div>Computation of the MAD is resilient to the presence of outliers.</div>
1591016559193	1516129351266	Standard normal distribution	Gaussian (aka normal) distribution, where the mean equals zero and the SD equals 1.<div><br></div><div>All Gaussian distributions can be converted to a standard normal distribution.</div>
1591016658074	1516129351266	Converting any Gaussian distribution to a standard normal distribution	Subtract the mean from each value and divide the difference by the SD.<div><br></div><div><img src="paste-df64ebe91f09c7960d1101253d75ebaba24923bf.jpg"><br></div>
1591016756246	1516129351266	The "central limit theorem"	<img src="paste-d2811a7641b7ab502b9f62c85387945636a66795.jpg">
1591016813961	1516129351266	Geometric mean (GM)	To compute a GM, first transform all the values to their logarithms and then calculate the mean of those logarithms. Finally, transform that mean of the logarithms back to the original units of the data.
1591016910081	1516129351266	Geometric SD	First compute the standard deviation of the logarithms. Then take the anti-logarithm (10 to the power) of that value.<div><br></div><div>The geometric SD must be multiplied times, or divided into, the GM.</div>
1591017023798	1516129351266	How to calculate the CI of a mean	<img src="paste-d7eb95d0310157e57aff5803234e9c1a8f43c295.jpg"><div><br></div><div>The CI covers the range defined by the sample mean m - W to m + W.</div><div><br></div><div>t* is a constant from the t distribution. It can be looked up in a table and depends on the degrees of freedom (= n - 1) and the desired level of confidence.</div><div><br></div><div><img src="paste-8709d5a6900263d967f1bc69bba5e875e9f2f4fa.jpg"><br></div>
1591017265114	1516129351266	Standard error of the mean (SEM)	<div><img src="paste-048ad5707a5a5005b13f151ca6dc1966ae256e80.jpg"><br></div><div><br></div><div>The SEM quantifies how precisely you know the population mean. It doesn't quantify variability.</div><div><br></div>It is sometimes called <i>standard error</i> for short.
1591034739050	1516129351266	P value	The P value answers this question: Assuming there is no effect in the overall population (the null hypothesis is true), what is the chance of observing an effect this size or larger?
1591034850954	1516129351266	Type 1 vs. Type 2 Error	<b><u>Type 1</u></b><div>Rejecting the null hypothesis, when it is actually true (false positive).</div><div><br></div><div><b><u>Type 2</u></b></div><div>Not rejecting the null hypothesis, when it is actually false (false negative).</div>
1591080100128	1516129351266	Relationship between P value, CI and statistical significance	If the 95% CI includes the value that defines the null hypothesis, you can conclude that the P value is greater than 0.05 and that the result is not statistically significant (assuming alpha = 5%).<div><br></div><div>If the 95% CI excludes the null hypothesis value, you can conclude that the P value is less than 0.05 and that the result is statistically significant (assuming alpha = 5%).</div><div><br></div><div>So if the 95% CI ends right at the value that defines the null hypothesis, then the P value must equal 0.05.</div>
1591081034117	1516129351266	Type S error	Also known as Type 3 error.<div><br></div><div>The conclusion is backward. You concluded that the drug (on average) significantly increases enzyme activity when in fact the drug decreases enzyme activity. You've correctly rejected the null hypothesis that the drug does not influence enzyme activity, so you have not made a Type 1 error. Instead, you have made a Type S error, because the sign (plus or minus, increase or decrease) of the actual overall effect is opposite to the result you happened to observe in one experiment.</div><div><br></div><div>Type S errors occur rarely but are not impossible.</div>
1591081194846	1516129351266	False Positive Report Probability (FPRP)	Also known as False Positive Risk (FPR) or False Discovery Rate (FDR).<div><br></div><div>Chance, that a statistically significant finding is actually due to a coincidence of random sampling. Answer to the question: "If you reject the null hypothesis, what is the chance you are wrong?".</div><div><br></div><div>It depends on the context of the study (prior probability that the hypothesis is true, based on prior data and theory).</div><div><br></div><div>FPRP = D/(C + D)</div><div><br></div><div><img src="paste-007ffb02faf38f79082313814a6e6cc0409507ac.jpg"><br></div>
1591195746161	1516129351266	Statistical Power	Answer to the question: "If there really were an effect of a specified value in the overall population, what is the chance of obtaining an effect that is statistically significant in one particular sample?"<div><br></div><div>Power = C/(C + D)</div><div><br></div><div><img src="paste-007ffb02faf38f79082313814a6e6cc0409507ac.jpg"><br></div>
1591196124437	1516129351266	Definition of alpha and beta	If the null hypothesis is true, then alpha is the chance of making the wrong decision (rejecting the null hypothesis).<div><br></div><div>alpha = A/(A + B)</div><div><br></div><div>If the null hypothesis is false (with a specified alternative hypothesis), beta is the chance of making the wrong decision (not rejecting the null hypothesis).</div><div><br></div><div>beta = D/(C + D)</div>
1591196282693	1516129351266	Relationship between beta and statistical power	beta is defined to equal 1.0 minus power
1591257229189	1516129351266	Bonferroni correction	Approach to achieve a familywise error rate (correcting for many comparisons).<div><br></div><div>Divide the value of alpha (often 5%) by the number of comparisons. Then define a particular comparison as statistically significant only when its P value is less than that ratio.</div>
